The performance drop after 200k episodes is mainly due to late-training instability combined with the high randomness of the environment. Although the agent converges earlier, continued training exposes it to rare and highly variable traffic patterns, cone layouts, and random starting lanes that it has not fully mastered. Because the reward function contains several interacting shaping terms, small policy updates can cause large behavioral changes when these uncommon scenarios appear, leading to sharp negative spikes.  Despite these occasional failures the curve remains stable.
